{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-15 08:18:18] INFO: Fetched (200) <GET http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60005.html> (referer: https://www.google.com/search?q=bom)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scrapling.engines.toolbelt.custom.Response'>\n"
     ]
    }
   ],
   "source": [
    "from scrapling import Fetcher\n",
    "page = Fetcher().get('http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60005.html', stealthy_headers=True, follow_redirects=True)\n",
    "\n",
    "#print the type of the page\n",
    "print(type(page))\n",
    "\n",
    "#convert page to string\n",
    "page = str(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Station_Name    Time_Day  Height Tendency  \\\n",
      "0     Lt Nerang Ck at Little Nerang Dam *  6.30am Wed  168.17   steady   \n",
      "1     Lt Nerang Ck at Little Nerang Dam #  5.46am Wed    0.16   steady   \n",
      "2           Nerang R at Numinbah Valley *  7.00am Wed    0.96   steady   \n",
      "3           Nerang R at Numinbah Valley #  5.41am Wed    0.95   steady   \n",
      "4                 Nerang R at Hinze Dam *  6.00am Wed   95.09   steady   \n",
      "...                                   ...         ...     ...      ...   \n",
      "1267                   Ugar Island Tide *  7.29am Wed    1.34   rising   \n",
      "1268                   Iama Island Tide *  7.34am Wed    0.75   steady   \n",
      "1269         Moa Island (St Pauls) Tide *  7.30am Wed    0.94   steady   \n",
      "1270            Moa Island (Kubin) Tide *  7.32am Wed    2.40   steady   \n",
      "1271               Thursday Island Tide *  7.35am Wed    1.66   steady   \n",
      "\n",
      "                 Crossing  Flood_Class IDQ_Number Station_ID  \\\n",
      "0     0.15 above Spillway                IDQ65388     540612   \n",
      "1     0.16 above Spillway  below minor   IDQ65388     540054   \n",
      "2                                        IDQ65388     540437   \n",
      "3                                        IDQ65388     540438   \n",
      "4     0.59 above Spillway  below minor   IDQ65388     540610   \n",
      "...                   ...          ...        ...        ...   \n",
      "1267       2.77 below HAT                IDQ65399     527022   \n",
      "1268       3.41 below HAT                IDQ65399     527018   \n",
      "1269       3.19 below HAT                IDQ65399     527021   \n",
      "1270       1.37 below HAT                IDQ65399     527017   \n",
      "1271       2.20 below HAT                IDQ65399     527005   \n",
      "\n",
      "               ReadingDateTime  \n",
      "0    2025-01-15 06:30:00+10:00  \n",
      "1    2025-01-15 05:46:00+10:00  \n",
      "2    2025-01-15 07:00:00+10:00  \n",
      "3    2025-01-15 05:41:00+10:00  \n",
      "4    2025-01-15 06:00:00+10:00  \n",
      "...                        ...  \n",
      "1267 2025-01-15 07:29:00+10:00  \n",
      "1268 2025-01-15 07:34:00+10:00  \n",
      "1269 2025-01-15 07:30:00+10:00  \n",
      "1270 2025-01-15 07:32:00+10:00  \n",
      "1271 2025-01-15 07:35:00+10:00  \n",
      "\n",
      "[1272 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "def parse_time_day_to_datetime(value, today_aest=None):\n",
    "    \"\"\"\n",
    "    Given a string like \"11.49pm Tue\", return a datetime in AEST for the *current week*.\n",
    "    We'll search backward up to 6 days if the day-of-week doesn't match \"today\".\n",
    "    \"\"\"\n",
    "\n",
    "    if not value or pd.isnull(value):\n",
    "        return pd.NaT  # or None\n",
    "\n",
    "    # If \"today_aest\" wasn't provided, use now in AEST:\n",
    "    if today_aest is None:\n",
    "        tz = ZoneInfo(\"Australia/Brisbane\")\n",
    "        today_aest = datetime.now(tz=tz)\n",
    "\n",
    "    # 1) Extract the time portion and the day name\n",
    "    # Typical format: \"HH.MM(am|pm) DayName\"\n",
    "    # e.g. \"11.49pm Tue\", \"6.29am Wed\"\n",
    "    match = re.match(r\"^(\\d{1,2})\\.(\\d{1,2})(am|pm)\\s+(\\w+)$\", value.strip(), re.IGNORECASE)\n",
    "    if not match:\n",
    "        # If it doesn't match our expected pattern, return NaT or None\n",
    "        return pd.NaT\n",
    "\n",
    "    hour_str, minute_str, ampm, day_str = match.groups()\n",
    "    hour = int(hour_str)\n",
    "    minute = int(minute_str)\n",
    "    ampm = ampm.lower()  # 'am' or 'pm'\n",
    "\n",
    "    # Convert to 24-hour format\n",
    "    if ampm == 'pm' and hour < 12:\n",
    "        hour += 12\n",
    "    elif ampm == 'am' and hour == 12:\n",
    "        hour = 0\n",
    "\n",
    "    # 2) Figure out which calendar date in the last 7 days has the correct day name\n",
    "    # Let's define a day-of-week name map, matching strftime(\"%a\") output: Mon, Tue, Wed, Thu, Fri, Sat, Sun\n",
    "    # Alternatively, you can do day_map = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"] and do an index approach.\n",
    "    # But let's do a simple \"search backwards\" approach.\n",
    "\n",
    "    tz = ZoneInfo(\"Australia/Brisbane\")\n",
    "    # We'll consider 'today' as the day we run the script, in AEST.\n",
    "    # We'll go from 0 to 6 days ago to find a date whose .strftime(\"%a\") = day_str\n",
    "    for offset in range(7):\n",
    "        candidate = today_aest - timedelta(days=offset)\n",
    "        if candidate.strftime(\"%a\") == day_str.title():  # e.g. \"Wed\" or \"Tue\"\n",
    "            # Found the matching day\n",
    "            # Now replace hour/minute to form the final reading datetime\n",
    "            reading_dt = datetime(\n",
    "                candidate.year,\n",
    "                candidate.month,\n",
    "                candidate.day,\n",
    "                hour,\n",
    "                minute,\n",
    "                tzinfo=tz\n",
    "            )\n",
    "            return reading_dt\n",
    "\n",
    "    # If we didn't find any match (unlikely if data is only 1-5 days old), fallback\n",
    "    return pd.NaT\n",
    "\n",
    "#convert page to string\n",
    "page_str = str(page)\n",
    "\n",
    "page_soup = BeautifulSoup(page_str, \"html.parser\")\n",
    "\n",
    "# Find the table (assuming there's only one table in the HTML)\n",
    "table = page_soup.find(\"table\")\n",
    "\n",
    "all_rows = []  # will hold lists of cell values\n",
    "\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    # Gather all cells (td/th) in this row\n",
    "    cells = tr.find_all([\"td\", \"th\"])\n",
    "    \n",
    "    # Skip row if any cell has a colspan\n",
    "    # (You could also check for rowspan if needed)\n",
    "    skip_row = any(cell.has_attr(\"colspan\") for cell in cells)\n",
    "    if skip_row:\n",
    "        continue\n",
    "\n",
    "    # Extract the text from each cell in the row\n",
    "    row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "    \n",
    "    # You mention the table has 7 columns, so only keep rows that have 7 cells\n",
    "    if len(row_data) == 7:\n",
    "        all_rows.append(row_data)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Look for the link that contains the word \"plot\" (case-insensitive)\n",
    "    # -------------------------------------------------------------------------\n",
    "    plot_link_tag = tr.find(\"a\", string=lambda text: text and \"plot\" in text.lower())\n",
    "    \n",
    "    # If found, parse the link's href to extract your IDQ number and second number\n",
    "    if plot_link_tag and plot_link_tag.has_attr(\"href\"):\n",
    "        href = plot_link_tag[\"href\"]  # e.g. http://www.bom.gov.au/fwo/IDQ65388/IDQ65388.540612.plt.shtml\n",
    "        \n",
    "        # Get the filename after the last slash, e.g. \"IDQ65388.540612.plt.shtml\"\n",
    "        filename = href.split(\"/\")[-1]\n",
    "        \n",
    "        # Use a regular expression to capture the part before the first '.' and the next part\n",
    "        # Pattern: ^(.*?)\\.(.*?)\\.plt\\.shtml$\n",
    "        match = re.match(r'^(.*?)\\.(.*?)\\.plt\\.shtml$', filename)\n",
    "        if match:\n",
    "            idq_number = match.group(1)     # e.g. \"IDQ65388\"\n",
    "            second_number = match.group(2)  # e.g. \"540612\"\n",
    "        else:\n",
    "            idq_number = None\n",
    "            second_number = None\n",
    "    else:\n",
    "        # If there is no link or no href attribute\n",
    "        idq_number = None\n",
    "        second_number = None\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Append these two new values into row_data\n",
    "    # -------------------------------------------------------------------------\n",
    "    row_data.append(idq_number)\n",
    "    row_data.append(second_number)\n",
    "\n",
    "\n",
    "# Now `all_rows` has 7 + 2 = 9 columns total:\n",
    "#   original 7 columns + IDQ_number + second_number\n",
    "\n",
    "# If the first row in `all_rows` is your header, then:\n",
    "header = all_rows[0]  # e.g. the original 7 column headers (if that's truly a header row)\n",
    "# You might want to adjust for the new columns in the header, for example:\n",
    "# header = header + [\"IDQ_Number\", \"Station_id\"]\n",
    "\n",
    "# Amend column titles to fill in missing values\n",
    "header[7] = \"IDQ_Number\"\n",
    "header[8] = \"Station_ID\"\n",
    "\n",
    "# Assuming the first row is the header\n",
    "df = pd.DataFrame(all_rows[1:], columns=all_rows[0])\n",
    "# df = pd.DataFrame(all_rows[1:], columns=header)\n",
    "\n",
    "# remove any instances of \"^\" from the height column\n",
    "df['Height'] = df['Height'].str.replace('^', '')\n",
    "\n",
    "# convert height column to float\n",
    "df['Height'] = df['Height'].astype(float)\n",
    "\n",
    "df[\"ReadingDateTime\"] = df[\"Time/Day\"].apply(parse_time_day_to_datetime)\n",
    "\n",
    "#Drop df['Recent Data'] column in place\n",
    "df.drop(columns=['Recent Data'], inplace=True)\n",
    "\n",
    "#For each column title, replace any spaces with _\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "# For Time/Day column replace the / in the title with an _\n",
    "df.columns = df.columns.str.replace('/', '_')\n",
    "\n",
    "print(df)\n",
    "\n",
    "#export to csv\n",
    "df.to_csv('flood.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
