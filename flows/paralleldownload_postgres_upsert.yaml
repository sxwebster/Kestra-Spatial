id: daily_firms_csv_update
namespace: workflows

tasks:
  - id: csv_data_download
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      # Download tasks for each dataset
      - id: dl_modis_c6
        type: io.kestra.plugin.core.http.Download
        uri: https://firms.modaps.eosdis.nasa.gov/data/active_fire/modis-c6.1/csv/MODIS_C6_1_Australia_NewZealand_24h.csv
        description: Download MODIS C6.1 dataset

      - id: dl_suomi_viirs_c2
        type: io.kestra.plugin.core.http.Download
        uri: https://firms.modaps.eosdis.nasa.gov/data/active_fire/suomi-npp-viirs-c2/csv/SUOMI_VIIRS_C2_Australia_NewZealand_24h.csv
        description: Download SUOMI VIIRS C2 dataset

      - id: dl_j1_viirs_c2
        type: io.kestra.plugin.core.http.Download
        uri: https://firms.modaps.eosdis.nasa.gov/data/active_fire/noaa-20-viirs-c2/csv/J1_VIIRS_C2_Australia_NewZealand_24h.csv
        description: Download J1 VIIRS C2 dataset

      - id: dl_j2_viirs_c2
        type: io.kestra.plugin.core.http.Download
        uri: https://firms.modaps.eosdis.nasa.gov/data/active_fire/noaa-21-viirs-c2/csv/J2_VIIRS_C2_Australia_NewZealand_24h.csv
        description: Download J2 VIIRS C2 dataset

  #
  # 1) Convert CSVs to Parquet & store the paths in parquet_paths.json
  #
  - id: convert_csvs_to_parquet
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    description: Convert all CSVs to Parquet
    outputFiles:
      - "*.json"
      - "*.parquet"
    script: |
      import pandas as pd
      import geopandas as gpd
      import json

      # Hypothetical inputs from previous tasks or from your Flow's inputs
      datasets = {
          "MODIS_C6_1": "{{ outputs.dl_modis_c6.uri }}",
          "SUOMI_VIIRS_C2": "{{ outputs.dl_suomi_viirs_c2.uri }}",
          "J1_VIIRS_C2": "{{ outputs.dl_j1_viirs_c2.uri }}",
          "J2_VIIRS_C2": "{{ outputs.dl_j2_viirs_c2.uri }}"
      }

      parquet_paths = {}

      for name, file_path in datasets.items():
          df = pd.read_csv(file_path)
          # Create geometry from longitude, latitude
          gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))

          # Convert to Parquet
          parquet_path = f"{name}.parquet"
          gdf.to_parquet(parquet_path, engine="pyarrow")
          parquet_paths[name] = parquet_path

      # Write out parquet_paths so we can read it in the next task
      with open("parquet_paths.json", "w") as f:
          json.dump(parquet_paths, f)

  #
  # 2) Read the Parquet files to GeoDataFrames & Bulk Upsert to PostGIS
  #
  - id: bulk-upsert-parquets
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    description: |
      1. Load the Parquet file paths from the previous task.
      2. For each Parquet, read into a GeoDataFrame.
      3. Bulk insert into a temp staging table.
      4. Upsert into the final table with ON CONFLICT (id).

    script: |
      import geopandas as gpd
      import pandas as pd
      import json
      import os
      from sqlalchemy import create_engine, MetaData
      from geoalchemy2 import Geometry
      
      import time
      from kestra import Kestra
      logger = Kestra.logger()
      logger.debug("DEBUG is used for diagnostic info.")
      time.sleep(0.5)

      # ------------------------------------------------------------------------------
      # 0. GET PARQUET PATHS FROM PREVIOUS TASK
      # ------------------------------------------------------------------------------
      # In Kestra, you can refer to the file we stored in the previous task:
      # convert_csvs_to_parquet
      parquet_paths = {
        "MODIS_C6_1": "MODIS_C6_1.parquet",
        "SUOMI_VIIRS_C2": "SUOMI_VIIRS_C2.parquet",
        "J1_VIIRS_C2": "J1_VIIRS_C2.parquet",
        "J2_VIIRS_C2": "J2_VIIRS_C2.parquet"
      }
      logger.info("This is a test log")

      # ------------------------------------------------------------------------------
      # 1. SET UP CONNECTION & TABLE NAMES
      # ------------------------------------------------------------------------------
      # Replace these with your real connection info
      engine = create_engine("postgresql://docker_user:docker_user@localhost:5433/template_postgis")

      final_table_name = "viirs_points"    # final PostGIS table
      temp_table_name = "viirs_points_tmp" # staging table

      metadata = MetaData()
      metadata.reflect(bind=engine)

      # ------------------------------------------------------------------------------
      # 2. CREATE FINAL TABLE IF IT DOES NOT EXIST
      # ------------------------------------------------------------------------------
      if final_table_name not in metadata.tables:
          # Create an empty table schema by writing an *empty* GeoDataFrame
          # or, you can manually execute a CREATE TABLE statement.
          # We'll pick one of the Parquet files for schema reference:
          sample_parquet = next(iter(parquet_paths.values()))
          sample_gdf = gpd.read_parquet(sample_parquet).head(0)

          # Make sure 'id' is your unique key or primary key (not automatically handled).
          # Also ensure geometry is of type POINT with an SRID of 4326:
          sample_gdf.to_postgis(
              name=final_table_name,
              con=engine,
              if_exists="replace",
              index=False,
              dtype={"geometry": Geometry("POINT", srid=4326)}
          )
          metadata.reflect(bind=engine)

      # ------------------------------------------------------------------------------
      # 3. FOR EACH PARQUET FILE: BULK UPSERT
      # ------------------------------------------------------------------------------
      for dataset_name, parquet_path in parquet_paths.items():
          gdf = gpd.read_parquet(parquet_path)

          # a) Drop the temp table if it exists
          with engine.begin() as conn:
              conn.execute(f"DROP TABLE IF EXISTS {temp_table_name}")

          # b) Bulk insert into the temp table
          gdf.to_postgis(
              name=temp_table_name,
              con=engine,
              if_exists="replace",
              index=False,
              dtype={"geometry": Geometry("POINT", srid=4326)}
          )

          # c) Single MERGE-like Upsert
          upsert_sql = f"""
          INSERT INTO {final_table_name} (id, name, geometry)
          SELECT id, name, geometry
          FROM {temp_table_name}
          ON CONFLICT (id) DO UPDATE
          SET
              name = EXCLUDED.name,
              geometry = EXCLUDED.geometry
          """
          with engine.begin() as conn:
              conn.execute(upsert_sql)

          
          # d) Optionally drop temp table to clean up each iteration
          with engine.begin() as conn:
              conn.execute(f"DROP TABLE IF EXISTS {temp_table_name}")
    
    
