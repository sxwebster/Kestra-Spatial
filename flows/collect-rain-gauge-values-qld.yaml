id: rain-gauge-levels-qld
namespace: workflows

triggers:
  - id: rain-gauge-qld-schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 */3 * * *"

tasks:
  - id: collect-rain-gauge-values-qld
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    
    script: |
        from scrapling import Fetcher
        import pandas as pd
        from bs4 import BeautifulSoup
        from datetime import datetime, timedelta, timezone
        import re
        import sqlalchemy
        from sqlalchemy import create_engine
        from kestra import Kestra
        import logging

        # Set Scraplings logging level to error to disable a default warning level error
        logging.getLogger("scrapling").setLevel(logging.ERROR)

        logger = Kestra.logger()

        # define a list of URLs
        urls = [
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60335.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60336.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60337.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60338.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60339.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60340.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60341.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60342.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60343.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60344.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60345.html",
            "http://www.bom.gov.au/cgi-bin/wrap_fwo.pl?IDQ60346.html"
        ]

        # Create the SQLAlchemy engine (this engine will be reused for each URL)
        engine = create_engine("postgresql://docker_user:docker_user@localhost:5432/docker_user")

        # Create the production table (if it doesn't already exist)
        create_prod_sql = """
        CREATE TABLE IF NOT EXISTS rain_data (
            station TEXT,
            river TEXT,
            timestamp_aest TIMESTAMPTZ,
            timestamp_utc TIMESTAMPTZ,
            value DOUBLE PRECISION,
            CONSTRAINT rain_data_pk PRIMARY KEY (station, river, timestamp_utc)
        );
        """
        with engine.begin() as conn:
            conn.exec_driver_sql(create_prod_sql)

        # Process each URL
        for url in urls:
            try:
                # --- Fetch and parse page ---
                page = Fetcher().get(url, stealthy_headers=True, follow_redirects=True)
                page = str(page).encode('utf-8')
                soup = BeautifulSoup(page, "html.parser")
                logger.debug(f"Encoding method: {soup.original_encoding}")

                # --- Extract issue time ---
                div_page = soup.find("div", id="page")
                p_tags = div_page.find_all("p")
                if len(p_tags) < 2:
                    raise ValueError("Cannot find at least two <p> elements in <div id='page'>")
                issue_text = p_tags[1].get_text(strip=True)
                logger.debug(f"Extracted issue_text: {issue_text}")

                pattern = r"Issued at\s+(\d{1,2}[.:]\d{1,2}\s*[ap]m)\s+on\s+(.+)"
                match = re.search(pattern, issue_text, re.IGNORECASE)
                if not match:
                    raise ValueError("Could not parse issue time text. Check the format and regex pattern.")

                time_str = match.group(1).replace('.', ':')
                date_str = match.group(2)
                dt_str = f"{time_str} {date_str}"
                issue_time_naive = datetime.strptime(dt_str, "%I:%M%p %A, %d %B %Y")
                aest = timezone(timedelta(hours=10))
                issue_time_aest = issue_time_naive.replace(tzinfo=aest)

                # --- Determine reading times ---
                first_table = soup.find("table", class_="tabledata rhb")
                thead = first_table.find("thead")
                header_rows = thead.find_all("tr")
                if len(header_rows) < 2:
                    raise ValueError("Expected at least two header rows")
                time_header_cells = header_rows[1].find_all("th")
                num_reading_cols = len(time_header_cells)
                last_reading_aest = issue_time_aest.replace(minute=0, second=0, microsecond=0)
                times_aest = [last_reading_aest - timedelta(hours=(num_reading_cols - 1 - i))
                            for i in range(num_reading_cols)]
                times_utc = [t.astimezone(timezone.utc) for t in times_aest]

                # --- Process tables and unpivot readings ---
                all_rows = []
                tables = soup.find_all("table", class_="tabledata rhb")
                for table in tables:
                    tbody = table.find("tbody")
                    tbody_rows = tbody.find_all("tr")
                    if len(tbody_rows) < 2:
                        continue
                    river_row = tbody_rows[0]
                    river = river_row.get_text(separator=" ", strip=True)
                    for row in tbody_rows[1:]:
                        cells = row.find_all(["th", "td"])
                        if len(cells) < 2:
                            continue
                        station = cells[0].get_text(separator=" ", strip=True)
                        reading_cells = cells[1:]
                        readings = []
                        for cell in reading_cells:
                            colspan = cell.get("colspan")
                            try:
                                span = int(colspan) if colspan else 1
                            except ValueError:
                                span = 1
                            cell_text = cell.get_text(strip=True)
                            if not cell_text or cell_text == "Â ":
                                value = None
                            else:
                                try:
                                    value = float(cell_text)
                                except ValueError:
                                    value = None
                            for _ in range(span):
                                readings.append(value)
                        if len(readings) != num_reading_cols:
                            logger.debug(f"Warning: Row for station {station} has {len(readings)} values, expected {num_reading_cols}.")
                        for ts_aest, ts_utc, val in zip(times_aest, times_utc, readings):
                            all_rows.append({
                                "station": station,
                                "river": river,
                                "timestamp_aest": ts_aest,
                                "timestamp_utc": ts_utc,
                                "value": val
                            })

                # Combine data into a pandas DataFrame
                df = pd.DataFrame(all_rows)
                df.sort_values(["river", "station", "timestamp_aest"], inplace=True)
                logger.debug(df)
                logger.debug(f"DataFrame columns: {df.columns}")

                # --- Remove duplicate rows based on the primary key columns ---
                df = df.drop_duplicates(subset=["station", "river", "timestamp_utc"])

                # --- Create a temporary table for staging data ---
                # Note: A true temporary table in PostgreSQL exists only within the current session.
                create_temp_sql = """
                CREATE TEMP TABLE temp_rain_data (
                    station TEXT,
                    river TEXT,
                    timestamp_aest TIMESTAMPTZ,
                    timestamp_utc TIMESTAMPTZ,
                    value DOUBLE PRECISION
                ) ON COMMIT DROP;
                """
                with engine.begin() as conn:
                    conn.exec_driver_sql(create_temp_sql)

                # Insert dataframe into the temporary staging table
                df.to_sql(
                    "temp_rain_data",
                    engine,
                    if_exists="append",
                    index=False
                )

                # --- Upsert from staging to production ---
                upsert_sql = """
                INSERT INTO rain_data (
                    station,
                    river,
                    timestamp_aest,
                    timestamp_utc,
                    value
                )
                SELECT DISTINCT station, river, timestamp_aest, timestamp_utc, value
                FROM temp_rain_data
                ON CONFLICT (station, river, timestamp_utc)
                DO UPDATE
                SET value = EXCLUDED.value,
                    timestamp_aest = EXCLUDED.timestamp_aest;
                """
                with engine.begin() as conn:
                    conn.exec_driver_sql(upsert_sql)

            except Exception as e:
                logger.debug(f"Error processing URL {url}: {e}")
                continue

            finally:
                # Ensure that any temporary table is dropped (in case it wasn't auto-dropped)
                try:
                    with engine.begin() as conn:
                        conn.exec_driver_sql("DROP TABLE IF EXISTS temp_rain_data")
                except Exception as drop_err:
                    logger.debug(f"Error dropping temp table after URL {url}: {drop_err}")
